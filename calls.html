---
id: calls
layout: default
style: true
---
<h2>Call for papers</h2>
<p>
Learning models defining recursive computations, like automata and formal grammars, are the core of the field called Grammatical Inference (GI). The expressive power of these models and the complexity of the associated computational problems are major research topics within mathematical logic and computer science. Historically, there has been little interaction between the GI and ICALP communities, though recently some important results started to bridge the gap between both worlds, including applications of learning to formal verification and model checking, and (co-)algebraic formulations of automata and grammar learning algorithms.
</p>
<p>
The aim of this workshop is to bring together experts on logic who could benefit from grammatical inference tools, and researchers in grammatical inference who could find in logic and verification new fruitful applications for their methods.
</p>
<p>
We invite submissions of recent work, including preliminary research, related to the theme of the workshop. The Program Committee will select a subset of the abstracts for oral presentation. At least one author of each accepted abstract is expected to represent it at the workshop.
</p>
<p>
Note that accepted papers will be made available on the workshop website but will not be part of formal proceedings (i.e., LearnAut is a non-archival workshop).
</p>

<div id="specifics">
    <div>
        <div id="topics">
            <h3>Topics of interest</h3>
            <p>
                <ul>
                    <li>Computational complexity of learning problems involving automata and formal languages.</li>
                    <li>Algorithms and frameworks for learning models representing language classes inside and outside the Chomsky hierarchy, including tree and graph grammars.</li>
                    <li>Learning problems involving models with additional structure, including numeric weights, inputs/outputs such as transducers, register automata, timed automata, Markov reward and decision processes, and semi-hidden Markov models.</li>
                    <li>Logical and relational aspects of learning and grammatical inference.</li>
                    <li>Theoretical studies of learnable classes of languages/representations.</li>
                    <li>Relations between automata or any other models from language theory and deep learning models for sequential data.</li>
                    <li>Active learning of finite state machines and formal languages.</li>
                    <li>Methods for estimating probability distributions over strings, trees, graphs, or any data used as input for symbolic models.</li>
                    <li>Applications of learning to formal verification and (statistical) model checking.</li>
                    <li>Metrics and other error measures between automata or formal languages.</li>
                </ul>
            </p>
        </div>

        <div>
            <div id="instructions">
                <h3>Submission instructions</h3>
                <p>
                    Submissions in the form of (preferably) anonymized extended abstracts must be at most 8 single-column pages long (excluding the bibliography and possible appendixes). We recommend using the JMLR/PMLR format. The LaTeX style file <a href="https://ctan.org/tex-archive/macros/latex/contrib/jmlr">is available here</a>.
                </p>

                <p>We do accept submissions of work recently published, currently under review or work-in-progress.</p>

                <p><a href="https://easychair.org/conferences/?conf=learnaut2024">EasyChair submission page</a></p>
            </div>

            <div id="dates">
                <h3>Important dates</h3>
                <ul>
                    {% for date in site.data.dates %}
                    <li>{{ date.name }}: {{ date.value }}</li>
                    {% endfor %}
                </ul>
            </div>
        </div>
    </div>
</div>
</div>
